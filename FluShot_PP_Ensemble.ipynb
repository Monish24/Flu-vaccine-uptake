{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Monish Shah Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('C:\\\\Masters Data Science\\\\Datasets\\\\training_set_features.csv')\n",
    "train_labels = pd.read_csv('C:\\\\Masters Data Science\\\\Datasets\\\\training_set_labels.csv')\n",
    "test_data = pd.read_csv('C:\\\\Masters Data Science\\\\Datasets\\\\test_set_features.csv')\n",
    "\n",
    "merged_data = pd.merge(train_data, train_labels, on=\"respondent_id\", how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling Missing Values in Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_data.loc[(merged_data['employment_status'] == 'Unemployed'), ['employment_industry', 'employment_occupation']] = ['Unemployed', 'Unemployed']\n",
    "#test_data.loc[(test_data['employment_status'] == 'Unemployed'), ['employment_industry', 'employment_occupation']] = ['Unemployed', 'Unemployed']\n",
    "\n",
    "#merged_data.loc[(merged_data['employment_status'] == 'Not in Labor Force'), ['employment_industry', 'employment_occupation']] = ['Not in Labor Force', 'Not in Labor Force']\n",
    "#test_data.loc[(test_data['employment_status'] == 'Not in Labor Force') , ['employment_industry', 'employment_occupation']] = ['Not in Labor Force', 'Not in Labor Force']\n",
    "\n",
    "#merged_data.loc[(merged_data['age_group'].isin(['65+ Years'])) & (merged_data['employment_status'].isnull()), ['employment_status']] = 'Not in Labor Force'\n",
    "#test_data.loc[(test_data['age_group'].isin(['65+ Years'])) & (test_data['employment_status'].isnull()), ['employment_status']] = 'Not in Labor Force'\n",
    "\n",
    "merged_data.loc[(merged_data['employment_occupation'].isnull() & (merged_data['employment_status'] == 'Unemployed')), 'employment_occupation'] = 'occupationUemp'\n",
    "merged_data.loc[(merged_data['employment_industry'].isnull() & (merged_data['employment_status'] == 'Unemployed')), 'employment_industry'] = 'industryUemp'\n",
    "test_data.loc[(test_data['employment_occupation'].isnull() & (test_data['employment_status'] == 'Unemployed')), 'employment_occupation'] = 'occupationUemp'\n",
    "test_data.loc[(test_data['employment_industry'].isnull() & (test_data['employment_status'] == 'Unemployed')), 'employment_industry'] = 'industryUemp'\n",
    "\n",
    "merged_data.loc[(merged_data['employment_industry'].isnull() & (merged_data['employment_status'] == 'Not in Labor Force')), 'employment_industry'] = 'industryna'\n",
    "merged_data.loc[(merged_data['employment_occupation'].isnull() & (merged_data['employment_status'] == 'Not in Labor Force')), 'employment_occupation'] = 'occupationna'\n",
    "test_data.loc[(test_data['employment_industry'].isnull() & (test_data['employment_status'] == 'Not in Labor Force')), 'employment_industry'] = 'industryna'\n",
    "test_data.loc[(test_data['employment_occupation'].isnull() & (test_data['employment_status'] == 'Not in Labor Force')), 'employment_occupation'] = 'occupationna'\n",
    "\n",
    "merged_data.loc[(merged_data['employment_industry'] == 'Unemployed') & (merged_data['employment_status'].isnull()), 'employment_industry'] = 'industryUemp'\n",
    "merged_data.loc[(merged_data['employment_industry'] == 'Unemployed') & (merged_data['employment_status'].isnull()), 'employment_occupation'] = 'occupationUemp'\n",
    "test_data.loc[(test_data['employment_industry'] == 'Unemployed') & (test_data['employment_status'].isnull()), 'employment_industry'] = 'industryUemp'\n",
    "test_data.loc[(test_data['employment_industry'] == 'Unemployed') & (test_data['employment_status'].isnull()), 'employment_occupation'] = 'occupationUemp'\n",
    "\n",
    "categorical_cols = ['education',\n",
    " 'race',\n",
    " 'sex',\n",
    " 'income_poverty',\n",
    " 'marital_status',\n",
    " 'rent_or_own',\n",
    " 'census_msa','employment_status','hhs_geo_region','age_group','employment_industry',\n",
    "                    'employment_occupation']\n",
    "\n",
    "merged_data[categorical_cols] = merged_data[categorical_cols].fillna('Missing')\n",
    "test_data[categorical_cols] = test_data[categorical_cols].fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding the Categorical and Ordinal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "cols_to_encode = ['education',\n",
    " 'race',\n",
    " 'sex',\n",
    " 'income_poverty',\n",
    " 'marital_status',\n",
    " 'rent_or_own',\n",
    " 'census_msa','employment_status','hhs_geo_region','age_group','employment_industry','employment_occupation']\n",
    "\n",
    "merged_data[cols_to_encode] = merged_data[cols_to_encode].apply(le.fit_transform)\n",
    "test_data[cols_to_encode] = test_data[cols_to_encode].apply(le.fit_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling Missing values in the remaining numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imputation function\n",
    "def fill_missing_values_mean(df, cols):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[cols] = imputer.fit_transform(df[cols])\n",
    "    return df\n",
    "\n",
    "# specify columns to impute\n",
    "cols_to_impute_mean = ['h1n1_concern', 'h1n1_knowledge','opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', \n",
    "                  'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_seas_risk', \n",
    "                  'opinion_seas_sick_from_vacc']\n",
    "\n",
    "# fill missing values with imputation for specified columns '', \n",
    "merged_data = fill_missing_values_mean(merged_data, cols_to_impute_mean)\n",
    "test_data = fill_missing_values_mean(test_data, cols_to_impute_mean)\n",
    "\n",
    "# define imputation function\n",
    "#def fill_missing_values_mean(df, cols):\n",
    "#    imputer = SimpleImputer(strategy='most_frequent')\n",
    "#    df[cols] = imputer.fit_transform(df[cols])\n",
    "#    return df\n",
    "\n",
    "# specify columns to impute\n",
    "#cols_to_impute_mean = ['behavioral_antiviral_meds', \n",
    "#                  'behavioral_avoidance',  'behavioral_face_mask',  'behavioral_wash_hands',  'behavioral_large_gatherings',  \n",
    "#                  'behavioral_outside_home',  'behavioral_touch_face','household_adults', 'household_children']\n",
    "\n",
    "# fill missing values with imputation for specified columns '', \n",
    "#merged_data = fill_missing_values_mean(merged_data, cols_to_impute_mean)\n",
    "#test_data = fill_missing_values_mean(test_data, cols_to_impute_mean)\n",
    "\n",
    "# define imputation function\n",
    "def fill_missing_values(df, cols):\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=-1)\n",
    "    df[cols] = imputer.fit_transform(df[cols])\n",
    "    return df\n",
    "\n",
    "# specify columns to impute\n",
    "cols_to_impute = ['health_insurance', 'behavioral_antiviral_meds', \n",
    "                  'behavioral_avoidance',  'behavioral_face_mask',  'behavioral_wash_hands',  'behavioral_large_gatherings',  \n",
    "                  'behavioral_outside_home',  'behavioral_touch_face',\n",
    "                  'doctor_recc_h1n1', 'doctor_recc_seasonal', 'chronic_med_condition', \n",
    "                  'child_under_6_months', 'health_worker','household_adults', 'household_children']\n",
    "\n",
    "# fill missing values with imputation for specified columns '', \n",
    "merged_data = fill_missing_values(merged_data, cols_to_impute)\n",
    "test_data = fill_missing_values(test_data, cols_to_impute)\n",
    "\n",
    "df_train = merged_data\n",
    "df_test = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods tried to Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_cols = ['education',\n",
    "# 'race',\n",
    "# 'sex',\n",
    "# 'income_poverty',\n",
    "# 'marital_status',\n",
    "# 'rent_or_own',\n",
    "# 'census_msa','employment_status','hhs_geo_region','age_group','employment_industry',\n",
    "#                    'employment_occupation']\n",
    "#\n",
    "#merged_data[categorical_cols] = merged_data[categorical_cols].fillna('Missing')\n",
    "#test_data[categorical_cols] = test_data[categorical_cols].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imputation function\n",
    "#def fill_missing_values(df, cols):\n",
    "#    imputer = SimpleImputer(strategy='most_frequent')\n",
    "#    df[cols] = imputer.fit_transform(df[cols])\n",
    "#    return df\n",
    "\n",
    "# specify columns to impute\n",
    "#cols_to_impute = ['employment_industry','employment_occupation','employment_status']\n",
    "\n",
    "# fill missing values with imputation for specified columns '',\n",
    "#merged_data = fill_missing_values(merged_data, cols_to_impute)\n",
    "#test_data = fill_missing_values(test_data, cols_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform BayesianRidge model-based imputation\n",
    "#imputer_merged_data = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42)\n",
    "#imputer_test_data = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42)\n",
    "\n",
    "# Impute the missing values in the training data and convert it back to a DataFrame\n",
    "#merged_data_imputed = imputer_merged_data.fit_transform(merged_data)\n",
    "#merged_data_imputed_df = pd.DataFrame(merged_data_imputed, columns=merged_data.columns, index=merged_data.index)\n",
    "\n",
    "# Impute the missing values in the test data using the same imputer and convert it back to a DataFrame\n",
    "#test_data_imputed = imputer_test_data.fit_transform(test_data)\n",
    "#test_data_imputed_df = pd.DataFrame(test_data_imputed, columns=test_data.columns, index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of respondents with health insurance for each employment status and income level\n",
    "#docc_recc_pct_h1n1 = merged_data.groupby(['chronic_med_condition'])['doctor_recc_h1n1'].transform('mean')\n",
    "#docc_recc_pct_seasonal = merged_data.groupby(['chronic_med_condition'])['doctor_recc_seasonal'].transform('mean')\n",
    "\n",
    "# Fill in missing values for health insurance based on the respondent's employment status and income level\n",
    "#merged_data['doctor_recc_h1n1'] = merged_data['doctor_recc_h1n1'].fillna(docc_recc_pct_h1n1)\n",
    "#test_data['doctor_recc_h1n1'] = test_data['doctor_recc_h1n1'].fillna(docc_recc_pct_h1n1)\n",
    "#merged_data['doctor_recc_seasonal'] = merged_data['doctor_recc_seasonal'].fillna(docc_recc_pct_seasonal)\n",
    "#test_data['doctor_recc_seasonal'] = test_data['doctor_recc_seasonal'].fillna(docc_recc_pct_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for imputation\n",
    "#features = ['h1n1_concern', 'h1n1_knowledge', 'behavioral_antiviral_meds', 'behavioral_avoidance', 'behavioral_face_mask', 'behavioral_wash_hands', 'behavioral_large_gatherings', 'behavioral_outside_home', 'behavioral_touch_face', 'doctor_recc_h1n1', 'doctor_recc_seasonal', 'chronic_med_condition', 'child_under_6_months', 'health_worker', 'age_group', 'education', 'race', 'sex', 'income_poverty', 'marital_status', 'rent_or_own', 'employment_status', 'hhs_geo_region', 'census_msa', 'household_adults', 'household_children', 'employment_industry', 'employment_occupation']\n",
    "#\n",
    "# Perform KNN imputation\n",
    "#imputer = KNNImputer(n_neighbors=5)\n",
    "#merged_data[features] = imputer.fit_transform(merged_data[features])\n",
    "\n",
    "# define imputation function\n",
    "#def fill_missing_values(df, cols):\n",
    "#    imputer = SimpleImputer(strategy='constant', fill_value=-1)\n",
    "#    df[cols] = imputer.fit_transform(df[cols])\n",
    "#    return df\n",
    "\n",
    "# specify columns to impute\n",
    "#cols_to_impute = ['health_insurance','h1n1_concern', 'h1n1_knowledge', 'behavioral_antiviral_meds', 'behavioral_avoidance',  'behavioral_face_mask',  'behavioral_wash_hands',  'behavioral_large_gatherings',  'behavioral_outside_home',  'behavioral_touch_face', 'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_seas_risk', 'opinion_seas_sick_from_vacc','doctor_recc_h1n1', 'doctor_recc_seasonal', 'chronic_med_condition', 'child_under_6_months', 'health_worker','household_adults', 'household_children']\n",
    "\n",
    "# fill missing values with imputation for specified columns '',\n",
    "#merged_data = fill_missing_values(merged_data, cols_to_impute)\n",
    "#est_data = fill_missing_values(test_data, cols_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with NaN\n",
    "#merged_data = merged_data.replace('?', np.nan)\n",
    "\n",
    "# Fill in missing values in the categorical column based on the related columns\n",
    "#mode_by_columns_roo = merged_data.groupby(['income_poverty', 'marital_status'])['rent_or_own'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['rent_or_own'] = merged_data.apply(lambda row: mode_by_columns_roo.get((row['income_poverty'], row['marital_status']), np.nan) if pd.isnull(row['rent_or_own']) else row['rent_or_own'], axis=1)\n",
    "#test_data['rent_or_own'] = test_data.apply(lambda row: mode_by_columns_roo.get((row['income_poverty'], row['marital_status']), np.nan) if pd.isnull(row['rent_or_own']) else row['rent_or_own'], axis=1)\n",
    "\n",
    "#mode_by_columns_ip = merged_data.groupby(['rent_or_own', 'marital_status'])['income_poverty'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['income_poverty'] = merged_data.apply(lambda row: mode_by_columns_ip.get((row['rent_or_own'], row['marital_status']), np.nan) if pd.isnull(row['income_poverty']) else row['income_poverty'], axis=1)\n",
    "#test_data['income_poverty'] = test_data.apply(lambda row: mode_by_columns_ip.get((row['rent_or_own'], row['marital_status']), np.nan) if pd.isnull(row['income_poverty']) else row['income_poverty'], axis=1)\n",
    "\n",
    "#mode_by_columns_ms = merged_data.groupby(['income_poverty', 'rent_or_own'])['marital_status'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['marital_status'] = merged_data.apply(lambda row: mode_by_columns_ms.get((row['income_poverty'], row['rent_or_own']), np.nan) if pd.isnull(row['marital_status']) else row['marital_status'], axis=1)\n",
    "#test_data['marital_status'] = test_data.apply(lambda row: mode_by_columns_ms.get((row['income_poverty'], row['rent_or_own']), np.nan) if pd.isnull(row['marital_status']) else row['marital_status'], axis=1)\n",
    "\n",
    "# Replace remaining NaN values with the mode of the entire column\n",
    "#merged_data['rent_or_own'].fillna(merged_data['rent_or_own'].mode()[0], inplace=True)\n",
    "#merged_data['income_poverty'].fillna(merged_data['income_poverty'].mode()[0], inplace=True)\n",
    "#merged_data['marital_status'].fillna(merged_data['marital_status'].mode()[0], inplace=True)\n",
    "#test_data['rent_or_own'].fillna(test_data['rent_or_own'].mode()[0], inplace=True)\n",
    "#test_data['income_poverty'].fillna(test_data['income_poverty'].mode()[0], inplace=True)\n",
    "#test_data['marital_status'].fillna(test_data['marital_status'].mode()[0], inplace=True)\n",
    "\n",
    "# Fill in missing values in the remaining categorical columns based on the related columns\n",
    "#mode_by_columns_edu = merged_data.groupby(['employment_status', 'employment_occupation'])['education'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['education'] = merged_data.apply(lambda row: mode_by_columns_edu.get((row['employment_status'], row['employment_occupation']), np.nan) if pd.isnull(row['education']) else row['education'], axis=1)\n",
    "#test_data['education'] = test_data.apply(lambda row: mode_by_columns_edu.get((row['employment_status'], row['employment_occupation']), np.nan) if pd.isnull(row['education']) else row['education'], axis=1)\n",
    "\n",
    "#mode_by_columns_emp = merged_data.groupby(['marital_status', 'age_group'])['employment_status'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['employment_status'] = merged_data.apply(lambda row: mode_by_columns_emp.get((row['marital_status'], row['age_group']), np.nan) if pd.isnull(row['employment_status']) else row['employment_status'], axis=1)\n",
    "#test_data['employment_status'] = test_data.apply(lambda row: mode_by_columns_emp.get((row['marital_status'], row['age_group']), np.nan) if pd.isnull(row['employment_status']) else row['employment_status'], axis=1)\n",
    "\n",
    "#mode_by_columns_ind = merged_data.groupby(['age_group', 'employment_status'])['employment_industry'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['employment_industry'] = merged_data.apply(lambda row: mode_by_columns_ind.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_industry']) else row['employment_industry'], axis=1)\n",
    "#test_data['employment_industry'] = test_data.apply(lambda row: mode_by_columns_ind.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_industry']) else row['employment_industry'], axis=1)\n",
    "\n",
    "#mode_by_columns_occ = merged_data.groupby(['age_group', 'employment_status'])['employment_occupation'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['employment_occupation'] = merged_data.apply(lambda row: mode_by_columns_occ.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_occupation']) else row['employment_occupation'], axis=1)\n",
    "#test_data['employment_occupation'] = test_data.apply(lambda row: mode_by_columns_occ.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_occupation']) else row['employment_occupation'], axis=1)\n",
    "\n",
    "#mode_by_columns_edu = merged_data.groupby(['employment_status', 'employment_occupation'])['education'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['education'] = merged_data.apply(lambda row: mode_by_columns_edu.get((row['employment_status'], row['employment_occupation']), np.nan) if pd.isnull(row['education']) else row['education'], axis=1)\n",
    "#test_data['education'] = test_data.apply(lambda row: mode_by_columns_edu.get((row['employment_status'], row['employment_occupation']), np.nan) if pd.isnull(row['education']) else row['education'], axis=1)\n",
    "\n",
    "#mode_by_columns_emp = merged_data.groupby(['marital_status', 'age_group'])['employment_status'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['employment_status'] = merged_data.apply(lambda row: mode_by_columns_emp.get((row['marital_status'], row['age_group']), np.nan) if pd.isnull(row['employment_status']) else row['employment_status'], axis=1)\n",
    "#test_data['employment_status'] = test_data.apply(lambda row: mode_by_columns_emp.get((row['marital_status'], row['age_group']), np.nan) if pd.isnull(row['employment_status']) else row['employment_status'], axis=1)\n",
    "\n",
    "#mode_by_columns_ind = merged_data.groupby(['age_group', 'employment_status'])['employment_industry'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['employment_industry'] = merged_data.apply(lambda row: mode_by_columns_ind.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_industry']) else row['employment_industry'], axis=1)\n",
    "#test_data['employment_industry'] = test_data.apply(lambda row: mode_by_columns_ind.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_industry']) else row['employment_industry'], axis=1)\n",
    "\n",
    "#mode_by_columns_occ = merged_data.groupby(['age_group', 'employment_status'])['employment_occupation'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "#merged_data['employment_occupation'] = merged_data.apply(lambda row: mode_by_columns_occ.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_occupation']) else row['employment_occupation'], axis=1)\n",
    "#test_data['employment_occupation'] = test_data.apply(lambda row: mode_by_columns_occ.get((row['age_group'], row['employment_status']), np.nan) if pd.isnull(row['employment_occupation']) else row['employment_occupation'], axis=1)\n",
    "\n",
    "# Replace remaining NaN values with the mode of the entire column\n",
    "#merged_data['education'].fillna(merged_data['education'].mode()[0], inplace=True)\n",
    "#merged_data['employment_status'].fillna(merged_data['employment_status'].mode()[0], inplace=True)\n",
    "#merged_data['household_adults'].fillna(merged_data['household_adults'].mode()[0], inplace=True)\n",
    "#merged_data['household_children'].fillna(merged_data['household_children'].mode()[0], inplace=True)\n",
    "#merged_data['employment_industry'].fillna(merged_data['employment_industry'].mode()[0], inplace=True)\n",
    "#merged_data['employment_occupation'].fillna(merged_data['employment_occupation'].mode()[0], inplace=True)\n",
    "#test_data['education'].fillna(test_data['education'].mode()[0], inplace=True)\n",
    "#test_data['employment_status'].fillna(test_data['employment_status'].mode()[0], inplace=True)\n",
    "#test_data['employment_industry'].fillna(test_data['employment_industry'].mode()[0], inplace=True)\n",
    "#test_data['household_adults'].fillna(test_data['household_adults'].mode()[0], inplace=True)\n",
    "#test_data['household_children'].fillna(test_data['household_children'].mode()[0], inplace=True)\n",
    "#test_data['employment_occupation'].fillna(test_data['employment_occupation'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficacy_cols = ['opinion_h1n1_vacc_effective', 'opinion_seas_vacc_effective']\n",
    "df_train['vaccine_efficacy_mean'] = df_train[efficacy_cols].mean(axis=1)\n",
    "df_test['vaccine_efficacy_mean'] = df_test[efficacy_cols].mean(axis=1)\n",
    "\n",
    "df_train['h1n1_concern_knowledge_interaction'] = df_train['h1n1_concern'] + df_train['h1n1_knowledge']\n",
    "df_test['h1n1_concern_knowledge_interaction'] = df_test['h1n1_concern'] + df_test['h1n1_knowledge']\n",
    "\n",
    "#def combine_columns(row):\n",
    "#    if row['doctor_recc_h1n1'] == -1 or row['vaccine_efficacy_mean'] == -1:\n",
    "#        h1n1_combined = -1\n",
    "#    else:\n",
    "#        h1n1_combined = row['doctor_recc_h1n1'] * row['vaccine_efficacy_mean']\n",
    "#        \n",
    "#    if row['doctor_recc_seasonal'] == -1 or row['vaccine_efficacy_mean'] == -1:\n",
    "#        seasonal_combined = -1\n",
    "#    else:\n",
    "#        seasonal_combined = row['doctor_recc_seasonal'] * row['vaccine_efficacy_mean']\n",
    "#        \n",
    "#    return pd.Series([h1n1_combined, seasonal_combined])\n",
    "#\n",
    "#df_train[['h1n1_combined', 'seasonal_combined']] = df_train.apply(combine_columns, axis=1)\n",
    "#df_test[['h1n1_combined', 'seasonal_combined']] = df_test.apply(combine_columns, axis=1)\n",
    "\n",
    "\n",
    "def create_missing_indicator_columns(df, cols):\n",
    "    for col in cols:\n",
    "        df[col + '_missing'] = df[col].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "cols_to_check = ['health_worker','employment_occupation', 'employment_industry', 'health_insurance','employment_status','income_poverty','doctor_recc_h1n1', 'doctor_recc_seasonal']  # Replace with your actual column names\n",
    "\n",
    "# Create the missing indicator columns for both merged_data and test_data\n",
    "merged_data = create_missing_indicator_columns(merged_data, cols_to_check)\n",
    "test_data = create_missing_indicator_columns(test_data, cols_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other new features that were created but did not improve the model (Different Combinations were tried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['doctor_recc_h1n1'] = df_train['doctor_recc_h1n1'].astype(bool)\n",
    "#df_train['doctor_recc_seasonal'] = df_train['doctor_recc_seasonal'].astype(bool)\n",
    "#df_train['doctor_recc'] = (df_train['doctor_recc_h1n1'] | df_train['doctor_recc_seasonal']).astype(int)\n",
    "#df_test['doctor_recc_h1n1'] = df_test['doctor_recc_h1n1'].astype(bool)\n",
    "#df_test['doctor_recc_seasonal'] = df_test['doctor_recc_seasonal'].astype(bool)\n",
    "#df_test['doctor_recc'] = (df_test['doctor_recc_h1n1'] | df_test['doctor_recc_seasonal']).astype(int)\n",
    "\n",
    "#df_train['doctor_recc_efficacy_interaction'] = df_train['doctor_recc'] * df_train['vaccine_efficacy_mean']\n",
    "#df_test['doctor_recc_efficacy_interaction'] = df_test['doctor_recc'] * df_test['vaccine_efficacy_mean']\n",
    "\n",
    "#df_train['h1n1_concern_knowledge_interaction'] = df_train['h1n1_concern'] * df_train['h1n1_knowledge']\n",
    "#df_test['h1n1_concern_knowledge_interaction'] = df_test['h1n1_concern'] * df_test['h1n1_knowledge']\n",
    "\n",
    "# New binary features\n",
    "#opinion_threshold = 3\n",
    "#binary_opinion_cols = ['opinion_h1n1_vacc_effective', 'opinion_seas_vacc_effective', 'opinion_h1n1_risk', 'opinion_seas_risk']\n",
    "\n",
    "#for col in binary_opinion_cols:\n",
    "#    new_col_name = f\"high_{col}_belief\"\n",
    "#    df_train[new_col_name] = (df_train[col] >= opinion_threshold).astype(int)\n",
    "#    df_test[new_col_name] = (df_test[col] >= opinion_threshold).astype(int)\n",
    "\n",
    "#def create_missing_indicator_columns(df, cols):\n",
    "#    for col in cols:\n",
    "#        df[col + '_missing'] = df[col].isna().astype(int)\n",
    "#    return df\n",
    "# List of columns for which you want to create missing value indicators\n",
    "#cols_to_check = ['opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_seas_risk', 'opinion_seas_sick_from_vacc','doctor_recc_h1n1', 'doctor_recc_seasonal']  # Replace with your actual column names\n",
    "\n",
    "# Create the missing indicator columns for both merged_data and test_data\n",
    "#merged_data = create_missing_indicator_columns(merged_data, cols_to_check)\n",
    "#test_data = create_missing_indicator_columns(test_data, cols_to_check)\n",
    "#merged_data['employment_type'] = merged_data['employment_industry'] + '_' + merged_data['employment_occupation']\n",
    "#test_data['employment_type'] = test_data['employment_industry'] + '_' + test_data['employment_occupation']\n",
    "\n",
    "# New features based on household composition\n",
    "#df_train['household_net'] = df_train['household_adults'] + df_train['household_children'] * 0.5\n",
    "#df_test['household_net'] = df_test['household_adults'] + df_test['household_children'] *0.5\n",
    "#df_train['adult_to_child_ratio'] = df_train['household_adults'] / (df_train['household_children'] + 1)\n",
    "#df_test['adult_to_child_ratio'] = df_test['household_adults'] / (df_test['household_children'] + 1)\n",
    "\n",
    "# New binary features\n",
    "#opinion_threshold = 3\n",
    "#binary_opinion_cols = ['opinion_h1n1_vacc_effective', 'opinion_seas_vacc_effective', 'opinion_h1n1_risk', 'opinion_seas_risk']\n",
    "#for col in binary_opinion_cols:\n",
    "#    new_col_name = f\"high_{col}_belief\"\n",
    "#    df_train[new_col_name] = (df_train[col] >= opinion_threshold).astype(int)\n",
    "#    df_test[new_col_name] = (df_test[col] >= opinion_threshold).astype(int)\n",
    "\n",
    "#df_train['Behavioral Precautions'] = df_train.iloc[:, 3:10].sum(axis=1)\n",
    "#df_train.drop(df_train.columns[3:10], axis=1, inplace=True)\n",
    "\n",
    "#df_train['age_chronic_condition'] = df_train['age_group'] * df_train['chronic_med_condition']\n",
    "#df_test['age_chronic_condition'] = df_test['age_group'] * df_test['chronic_med_condition']\n",
    "\n",
    "#df_train['health_worker_opinion_h1n1_risk'] = df_train['health_worker'] * df_train['opinion_h1n1_risk']\n",
    "#df_train['health_worker_opinion_seas_risk'] = df_train['health_worker'] * df_train['opinion_seas_risk']\n",
    "#df_test['health_worker_opinion_h1n1_risk'] = df_test['health_worker'] * df_test['opinion_h1n1_risk']\n",
    "#df_test['health_worker_opinion_seas_risk'] = df_test['health_worker'] * df_test['opinion_seas_risk']\n",
    "\n",
    "#df_train['age_health_insurance'] = df_train['age_group'] * df_train['health_insurance']\n",
    "#df_test['age_health_insurance'] = df_test['age_group'] * df_test['health_insurance']\n",
    "\n",
    "# Interaction between demographic features and opinions/behaviors\n",
    "#demo_cols = ['income_poverty', 'employment_status', 'education']\n",
    "#opinion_cols = ['opinion_h1n1_vacc_effective', 'opinion_seas_vacc_effective', 'opinion_h1n1_risk', 'opinion_seas_risk']\n",
    "#for demo_col in demo_cols:\n",
    "#    for opinion_col in opinion_cols:\n",
    "#        new_col_name = f\"{demo_col}_{opinion_col}_interaction\"\n",
    "#        df_train[new_col_name] = df_train[demo_col] * df_train[opinion_col]\n",
    "#        df_test[new_col_name] = df_test[demo_col] * df_test[opinion_col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['respondent_id'],axis=1)\n",
    "X_test = df_test.drop(['respondent_id'], axis=1)\n",
    "\n",
    "# Split data into X and y\n",
    "X_train = df_train.drop(['h1n1_vaccine', 'seasonal_vaccine'],axis=1)\n",
    "y_train_h1n1 = df_train['h1n1_vaccine']\n",
    "y_train_seasonal = df_train['seasonal_vaccine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Ensemble using LGBoost, XGBoost, CatBoost and GradientBoosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models for h1n1_vaccine...\n",
      "Training LGBoost Model...\n",
      "LGBoost Model AUC score: 0.8713918138164658\n",
      "Training XGBoost Model...\n",
      "XGBoost Model AUC score: 0.8717068069809766\n",
      "Training CatBoost Model...\n",
      "CatBoost Model AUC score: 0.8724300478952399\n",
      "Training GBM Model...\n",
      "GBM Model AUC score: 0.8691264557293742\n",
      "Training models for seasonal_vaccine...\n",
      "Training LGBoost Model...\n",
      "LGBoost Model AUC score: 0.8652760631934682\n",
      "Training XGBoost Model...\n",
      "XGBoost Model AUC score: 0.8653634630555794\n",
      "Training CatBoost Model...\n",
      "CatBoost Model AUC score: 0.8647477631594558\n",
      "Training GBM Model...\n",
      "GBM Model AUC score: 0.8642909969548771\n",
      "AUC Scores...\n",
      "H1N1 =  0.8711637811055142\n",
      "Seasonal =  0.8649195715908451\n",
      "Ensemble Model =  0.8680416763481796\n",
      "Making predictions using ensemble models...\n",
      "Saving predictions to CSV file...\n"
     ]
    }
   ],
   "source": [
    "#GBM model for h1n1_vaccine\n",
    "gbm_model_h1n1 = GradientBoostingClassifier(n_estimators=500, max_depth=7, learning_rate=0.03,\n",
    "min_samples_split=2, min_samples_leaf=1, subsample=0.9,\n",
    "max_features='sqrt', random_state=42)\n",
    "\n",
    "#GBM model for seasonal_vaccine\n",
    "gbm_model_seasonal = GradientBoostingClassifier(n_estimators=500, max_depth=5, learning_rate=0.03,\n",
    "min_samples_split=3, min_samples_leaf=3, subsample=0.6,\n",
    "max_features='sqrt', random_state=42)\n",
    "\n",
    "# LightGBM model for h1n1_vaccine\n",
    "lgb_model_h1n1 = lgb.LGBMClassifier(device='CPU',objective='binary', learning_rate=0.03, n_estimators=500, max_depth=6,\n",
    "                                    min_child_weight=2, subsample=0.9, colsample_bytree=0.5)\n",
    "\n",
    "# XGBoost model for h1n1_vaccine\n",
    "xgb_model_h1n1 = xgb.XGBClassifier(tree_method='auto',objective='binary:logistic', learning_rate=0.03, n_estimators=500, max_depth=5,\n",
    "                                    min_child_weight=2, subsample=0.9, colsample_bytree=0.5)\n",
    "\n",
    "# CatBoost model for h1n1_vaccine\n",
    "cb_model_h1n1 = cb.CatBoostClassifier(objective='Logloss', learning_rate=0.03, n_estimators=1000, max_depth=6,\n",
    "                                    min_child_samples=2, subsample=0.9, colsample_bylevel=0.5,\n",
    "                                    early_stopping_rounds=10, verbose=0,cat_features=['education', 'race','sex', 'income_poverty',\n",
    "                                                        'marital_status', 'rent_or_own', 'census_msa',\n",
    "                                                        'employment_industry', 'employment_occupation',\n",
    "                                                        'employment_status', 'hhs_geo_region', 'age_group'])\n",
    "\n",
    "# LightGBM model for seasonal_vaccine\n",
    "lgb_model_seasonal = lgb.LGBMClassifier(device='CPU',objective='binary', learning_rate=0.03, n_estimators=500, max_depth=5,\n",
    "                                       min_child_weight=5, subsample=0.6, colsample_bytree=0.6)\n",
    "\n",
    "# XGBoost model for seasonal_vaccine\n",
    "xgb_model_seasonal = xgb.XGBClassifier(tree_method='auto',objective='binary:logistic', learning_rate=0.022, n_estimators=700, max_depth=5,\n",
    "                                   min_child_weight=2, subsample=0.6, colsample_bytree=0.6)\n",
    "\n",
    "# CatBoost model for seasonal_vaccine\n",
    "cb_model_seasonal = cb.CatBoostClassifier(objective='Logloss', learning_rate=0.03, n_estimators=1000, max_depth=5,\n",
    "                                   min_child_samples=5, subsample=0.6, colsample_bylevel=0.6,\n",
    "                                   early_stopping_rounds=10, verbose=0,cat_features=['education', 'race','sex', 'income_poverty',\n",
    "                                                        'marital_status', 'rent_or_own', 'census_msa',\n",
    "                                                        'employment_industry', 'employment_occupation',\n",
    "                                                        'employment_status', 'hhs_geo_region', 'age_group'])\n",
    "\n",
    "# Train models for h1n1_vaccine\n",
    "print(\"Training models for h1n1_vaccine...\")\n",
    "print(\"Training LGBoost Model...\")\n",
    "lgb_score_h1n1 = np.mean(cross_val_score(lgb_model_h1n1, X_train, y_train_h1n1, cv=5, scoring='roc_auc'))\n",
    "print(\"LGBoost Model AUC score:\", lgb_score_h1n1)\n",
    "lgb_model_h1n1.fit(X_train, y_train_h1n1)\n",
    "print(\"Training XGBoost Model...\")\n",
    "xgb_score_h1n1 = np.mean(cross_val_score(xgb_model_h1n1, X_train, y_train_h1n1, cv=5, scoring='roc_auc'))\n",
    "print(\"XGBoost Model AUC score:\", xgb_score_h1n1)\n",
    "xgb_model_h1n1.fit(X_train, y_train_h1n1)\n",
    "print(\"Training CatBoost Model...\")\n",
    "cb_score_h1n1 = np.mean(cross_val_score(cb_model_h1n1, X_train, y_train_h1n1, cv=5, scoring='roc_auc'))\n",
    "print(\"CatBoost Model AUC score:\", cb_score_h1n1)\n",
    "cb_model_h1n1.fit(X_train, y_train_h1n1)\n",
    "print(\"Training GBM Model...\")\n",
    "gbm_score_h1n1 = np.mean(cross_val_score(gbm_model_h1n1, X_train, y_train_h1n1, cv=5, scoring='roc_auc'))\n",
    "print(\"GBM Model AUC score:\", gbm_score_h1n1)\n",
    "gbm_model_h1n1.fit(X_train, y_train_h1n1)\n",
    "\n",
    "# Train models for seasonal_vaccine\n",
    "print(\"Training models for seasonal_vaccine...\")\n",
    "print(\"Training LGBoost Model...\")\n",
    "lgb_score_seasonal = np.mean(cross_val_score(lgb_model_seasonal, X_train, y_train_seasonal, cv=5, scoring='roc_auc'))\n",
    "print(\"LGBoost Model AUC score:\", lgb_score_seasonal)\n",
    "lgb_model_seasonal.fit(X_train, y_train_seasonal)\n",
    "print(\"Training XGBoost Model...\")\n",
    "xgb_score_seasonal = np.mean(cross_val_score(xgb_model_seasonal, X_train, y_train_seasonal, cv=5, scoring='roc_auc'))\n",
    "print(\"XGBoost Model AUC score:\", xgb_score_seasonal)\n",
    "xgb_model_seasonal.fit(X_train, y_train_seasonal)\n",
    "print(\"Training CatBoost Model...\")\n",
    "cb_score_seasonal = np.mean(cross_val_score(cb_model_seasonal, X_train, y_train_seasonal, cv=5, scoring='roc_auc'))\n",
    "print(\"CatBoost Model AUC score:\", cb_score_seasonal)\n",
    "cb_model_seasonal.fit(X_train, y_train_seasonal)\n",
    "print(\"Training GBM Model...\")\n",
    "gbm_score_seasonal = np.mean(cross_val_score(gbm_model_seasonal, X_train, y_train_seasonal, cv=5, scoring='roc_auc'))\n",
    "print(\"GBM Model AUC score:\", gbm_score_seasonal)\n",
    "gbm_model_seasonal.fit(X_train, y_train_seasonal)\n",
    "\n",
    "overallmean_h1n1 = (lgb_score_h1n1+xgb_score_h1n1+cb_score_h1n1+gbm_score_h1n1) / 4\n",
    "overallmean_seasonal = (lgb_score_seasonal+xgb_score_seasonal+cb_score_seasonal+gbm_score_seasonal) / 4\n",
    "overallmean = (overallmean_h1n1+overallmean_seasonal)/2\n",
    "\n",
    "print(\"AUC Scores...\")\n",
    "print(\"H1N1 = \",overallmean_h1n1)\n",
    "print(\"Seasonal = \",overallmean_seasonal)\n",
    "print(\"Ensemble Model = \",overallmean)\n",
    "\n",
    "# Make predictions using each model\n",
    "print(\"Making predictions using ensemble models...\")\n",
    "lgb_pred_h1n1 = lgb_model_h1n1.predict_proba(X_test)[:, 1]\n",
    "xgb_pred_h1n1 = xgb_model_h1n1.predict_proba(X_test)[:, 1]\n",
    "cb_pred_h1n1 = cb_model_h1n1.predict_proba(X_test)[:, 1]\n",
    "gbm_pred_h1n1 = gbm_model_h1n1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "lgb_pred_seasonal = lgb_model_seasonal.predict_proba(X_test)[:, 1]\n",
    "xgb_pred_seasonal = xgb_model_seasonal.predict_proba(X_test)[:, 1]\n",
    "cb_pred_seasonal = cb_model_seasonal.predict_proba(X_test)[:, 1]\n",
    "gbm_pred_seasonal = gbm_model_seasonal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "weighted_pred_h1n1 = ((0.2*lgb_pred_h1n1) +(0.3*xgb_pred_h1n1) + (0.25*cb_pred_h1n1) + (0.2*gbm_pred_h1n1)) / 4\n",
    "weighted_pred_seasonal = ((0.25*lgb_pred_seasonal) + (0.3*xgb_pred_seasonal) + (0.2*cb_pred_seasonal) + (0.2*gbm_pred_seasonal)) / 4\n",
    "\n",
    "# Save predictions to CSV file\n",
    "print(\"Saving predictions to CSV file...\")\n",
    "ensemble_df = pd.DataFrame({'respondent_id': df_test['respondent_id'],\n",
    "                            'h1n1_vaccine': weighted_pred_h1n1,\n",
    "                            'seasonal_vaccine': weighted_pred_seasonal})\n",
    "ensemble_df.to_csv('ensemble_predictions_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
